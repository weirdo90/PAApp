{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DDPG_continuous(GrindingCircuit)_pytorch.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "REKIeWYWnU-P",
        "EwWajEMtfovE"
      ],
      "toc_visible": true,
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/woipi90/PAApp/blob/master/DDPG_continuous(GrindingCircuit)_pytorch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "REKIeWYWnU-P"
      },
      "source": [
        "# Import"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GtNLYk3zFhf_"
      },
      "source": [
        "# Install gym patch for continuous action space example in COLAB\n",
        "#!pip install gym[box2d]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zvAYpz7Wd3ef"
      },
      "source": [
        "# Access google drive\n",
        "#from google.colab import drive\n",
        "#drive.mount('/content/gdrive/')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N8sKPtDIhe7C"
      },
      "source": [
        "# Navigate to drive folder in order to access files\n",
        "#import sys\n",
        "#sys.path.insert(0,'/content/gdrive/My Drive/Colab Notebooks/DDPG')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iMm2FZSPyhQ6"
      },
      "source": [
        "# RL imports\n",
        "import gym\n",
        "import numpy as np\n",
        "#from ddpg_torch import Agent\n",
        "#from utils import plotLearning\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "import torch as T\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "\n",
        "# Customized environment imports\n",
        "from gym import Env\n",
        "from gym.spaces import Box\n",
        "import random\n",
        "from gym.utils import seeding\n",
        "from gym import spaces"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EwWajEMtfovE"
      },
      "source": [
        "# DDPG"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MAyGpXnEgBxi"
      },
      "source": [
        "class OUActionNoise(object):\n",
        "  def __init__(self, mu, sigma=0.15, theta=.2, dt=1e-2, x0=None):\n",
        "    self.theta = theta\n",
        "    self.mu = mu\n",
        "    self.sigma = sigma\n",
        "    self.dt = dt\n",
        "    self.x0 = x0\n",
        "    self.reset()\n",
        "\n",
        "  def __call__(self):\n",
        "    x = self.x_prev + self.theta * (self.mu - self.x_prev) * self.dt + \\\n",
        "        self.sigma * np.sqrt(self.dt) * np.random.normal(size=self.mu.shape)\n",
        "    self.x_prev = x\n",
        "    return x\n",
        "\n",
        "  def reset(self):\n",
        "    self.x_prev = self.x0 if self.x0 is not None else np.zeros_like(self.mu)\n",
        "\n",
        "  def __repr__(self):\n",
        "    return 'OrnsteinUhlenbeckActionNoise(mu={}, sigma={})'.format(\n",
        "                                                            self.mu, self.sigma)\n",
        "\n",
        "class ReplayBuffer(object):\n",
        "  def __init__(self, max_size, input_shape, n_actions):\n",
        "    self.mem_size = max_size\n",
        "    self.mem_cntr = 0\n",
        "    self.state_memory = np.zeros((self.mem_size, *input_shape))\n",
        "    self.new_state_memory = np.zeros((self.mem_size, *input_shape))\n",
        "    self.action_memory = np.zeros((self.mem_size, n_actions))\n",
        "    self.reward_memory = np.zeros(self.mem_size)\n",
        "    self.terminal_memory = np.zeros(self.mem_size, dtype=np.float32)\n",
        "\n",
        "  def store_transition(self, state, action, reward, state_, done):\n",
        "    index = self.mem_cntr % self.mem_size\n",
        "    self.state_memory[index] = state\n",
        "    self.new_state_memory[index] = state_\n",
        "    self.action_memory[index] = action\n",
        "    self.reward_memory[index] = reward\n",
        "    self.terminal_memory[index] = 1 - done\n",
        "    self.mem_cntr += 1\n",
        "\n",
        "  def sample_buffer(self, batch_size):\n",
        "    max_mem = min(self.mem_cntr, self.mem_size)\n",
        "\n",
        "    batch = np.random.choice(max_mem, batch_size)\n",
        "\n",
        "    states = self.state_memory[batch]\n",
        "    actions = self.action_memory[batch]\n",
        "    rewards = self.reward_memory[batch]\n",
        "    states_ = self.new_state_memory[batch]\n",
        "    terminal = self.terminal_memory[batch]\n",
        "\n",
        "    return states, actions, rewards, states_, terminal\n",
        "\n",
        "class CriticNetwork(nn.Module):\n",
        "  def __init__(self, beta, input_dims, fc1_dims, fc2_dims, n_actions, name,\n",
        "                chkpt_dir='tmp/ddpg'):\n",
        "    super(CriticNetwork, self).__init__()\n",
        "    self.input_dims = input_dims\n",
        "    self.fc1_dims = fc1_dims\n",
        "    self.fc2_dims = fc2_dims\n",
        "    self.n_actions = n_actions\n",
        "    self.checkpoint_file = os.path.join(chkpt_dir,name+'_ddpg')\n",
        "    self.fc1 = nn.Linear(*self.input_dims, self.fc1_dims)\n",
        "    f1 = 1./np.sqrt(self.fc1.weight.data.size()[0])\n",
        "    T.nn.init.uniform_(self.fc1.weight.data, -f1, f1)\n",
        "    T.nn.init.uniform_(self.fc1.bias.data, -f1, f1)\n",
        "    #self.fc1.weight.data.uniform_(-f1, f1)\n",
        "    #self.fc1.bias.data.uniform_(-f1, f1)\n",
        "    self.bn1 = nn.LayerNorm(self.fc1_dims)\n",
        "\n",
        "    self.fc2 = nn.Linear(self.fc1_dims, self.fc2_dims)\n",
        "    f2 = 1./np.sqrt(self.fc2.weight.data.size()[0])\n",
        "    #f2 = 0.002\n",
        "    T.nn.init.uniform_(self.fc2.weight.data, -f2, f2)\n",
        "    T.nn.init.uniform_(self.fc2.bias.data, -f2, f2)\n",
        "    #self.fc2.weight.data.uniform_(-f2, f2)\n",
        "    #self.fc2.bias.data.uniform_(-f2, f2)\n",
        "    self.bn2 = nn.LayerNorm(self.fc2_dims)\n",
        "\n",
        "    self.action_value = nn.Linear(self.n_actions, self.fc2_dims)\n",
        "    f3 = 0.003\n",
        "    self.q = nn.Linear(self.fc2_dims, 1)\n",
        "    T.nn.init.uniform_(self.q.weight.data, -f3, f3)\n",
        "    T.nn.init.uniform_(self.q.bias.data, -f3, f3)\n",
        "    #self.q.weight.data.uniform_(-f3, f3)\n",
        "    #self.q.bias.data.uniform_(-f3, f3)\n",
        "\n",
        "    self.optimizer = optim.Adam(self.parameters(), lr=beta)\n",
        "    self.device = T.device('cuda:0' if T.cuda.is_available() else 'cpu')\n",
        "\n",
        "    self.to(self.device)\n",
        "\n",
        "  def forward(self, state, action):\n",
        "    state_value = self.fc1(state)\n",
        "    state_value = self.bn1(state_value)\n",
        "    state_value = F.relu(state_value)\n",
        "    state_value = self.fc2(state_value)\n",
        "    state_value = self.bn2(state_value)\n",
        "\n",
        "    action_value = F.relu(self.action_value(action))\n",
        "    state_action_value = F.relu(T.add(state_value, action_value))\n",
        "    state_action_value = self.q(state_action_value)\n",
        "\n",
        "    return state_action_value\n",
        "\n",
        "  def save_checkpoint(self):\n",
        "    print('... saving checkpoint ...')\n",
        "    T.save(self.state_dict(), self.checkpoint_file)\n",
        "\n",
        "  def load_checkpoint(self):\n",
        "    print('... loading checkpoint ...')\n",
        "    self.load_state_dict(T.load(self.checkpoint_file))\n",
        "\n",
        "class ActorNetwork(nn.Module):\n",
        "  def __init__(self, alpha, input_dims, fc1_dims, fc2_dims, n_actions, name,\n",
        "                chkpt_dir='tmp/ddpg'):\n",
        "    super(ActorNetwork, self).__init__()\n",
        "    self.input_dims = input_dims\n",
        "    self.fc1_dims = fc1_dims\n",
        "    self.fc2_dims = fc2_dims\n",
        "    self.n_actions = n_actions\n",
        "    self.checkpoint_file = os.path.join(chkpt_dir,name+'_ddpg')\n",
        "    self.fc1 = nn.Linear(*self.input_dims, self.fc1_dims)\n",
        "    f1 = 1./np.sqrt(self.fc1.weight.data.size()[0])\n",
        "    T.nn.init.uniform_(self.fc1.weight.data, -f1, f1)\n",
        "    T.nn.init.uniform_(self.fc1.bias.data, -f1, f1)\n",
        "    #self.fc1.weight.data.uniform_(-f1, f1)\n",
        "    #self.fc1.bias.data.uniform_(-f1, f1)\n",
        "    self.bn1 = nn.LayerNorm(self.fc1_dims)\n",
        "\n",
        "    self.fc2 = nn.Linear(self.fc1_dims, self.fc2_dims)\n",
        "    #f2 = 0.002\n",
        "    f2 = 1./np.sqrt(self.fc2.weight.data.size()[0])\n",
        "    T.nn.init.uniform_(self.fc2.weight.data, -f2, f2)\n",
        "    T.nn.init.uniform_(self.fc2.bias.data, -f2, f2)\n",
        "    #self.fc2.weight.data.uniform_(-f2, f2)\n",
        "    #self.fc2.bias.data.uniform_(-f2, f2)\n",
        "    self.bn2 = nn.LayerNorm(self.fc2_dims)\n",
        "\n",
        "    #f3 = 0.004\n",
        "    f3 = 0.003\n",
        "    self.mu = nn.Linear(self.fc2_dims, self.n_actions)\n",
        "    T.nn.init.uniform_(self.mu.weight.data, -f3, f3)\n",
        "    T.nn.init.uniform_(self.mu.bias.data, -f3, f3)\n",
        "    #self.mu.weight.data.uniform_(-f3, f3)\n",
        "    #self.mu.bias.data.uniform_(-f3, f3)\n",
        "\n",
        "    self.optimizer = optim.Adam(self.parameters(), lr=alpha)\n",
        "    self.device = T.device('cuda:0' if T.cuda.is_available() else 'cpu')\n",
        "\n",
        "    self.to(self.device)\n",
        "\n",
        "  def forward(self, state):\n",
        "    x = self.fc1(state)\n",
        "    x = self.bn1(x)\n",
        "    x = F.relu(x)\n",
        "    x = self.fc2(x)\n",
        "    x = self.bn2(x)\n",
        "    x = F.relu(x)\n",
        "    x = T.tanh(self.mu(x))\n",
        "\n",
        "    return x\n",
        "\n",
        "  def save_checkpoint(self):\n",
        "    print('... saving checkpoint ...')\n",
        "    T.save(self.state_dict(), self.checkpoint_file)\n",
        "\n",
        "  def load_checkpoint(self):\n",
        "    print('... loading checkpoint ...')\n",
        "    self.load_state_dict(T.load(self.checkpoint_file))\n",
        "\n",
        "class Agent(object):\n",
        "  def __init__(self, alpha, beta, input_dims, tau, env, gamma=0.99,\n",
        "                n_actions=2, max_size=1000000, layer1_size=400,\n",
        "                layer2_size=300, batch_size=64):\n",
        "    self.gamma = gamma\n",
        "    self.tau = tau\n",
        "    self.memory = ReplayBuffer(max_size, input_dims, n_actions)\n",
        "    self.batch_size = batch_size\n",
        "\n",
        "    self.actor = ActorNetwork(alpha, input_dims, layer1_size,\n",
        "                              layer2_size, n_actions=n_actions,\n",
        "                              name='Actor')\n",
        "    self.critic = CriticNetwork(beta, input_dims, layer1_size,\n",
        "                                layer2_size, n_actions=n_actions,\n",
        "                                name='Critic')\n",
        "\n",
        "    self.target_actor = ActorNetwork(alpha, input_dims, layer1_size,\n",
        "                                      layer2_size, n_actions=n_actions,\n",
        "                                      name='TargetActor')\n",
        "    self.target_critic = CriticNetwork(beta, input_dims, layer1_size,\n",
        "                                        layer2_size, n_actions=n_actions,\n",
        "                                        name='TargetCritic')\n",
        "\n",
        "    self.noise = OUActionNoise(mu=np.zeros(n_actions))\n",
        "\n",
        "    self.update_network_parameters(tau=1)\n",
        "\n",
        "  def choose_action(self, observation):\n",
        "    self.actor.eval()\n",
        "    observation = T.tensor(observation, dtype=T.float).to(self.actor.device)\n",
        "    mu = self.actor.forward(observation).to(self.actor.device)\n",
        "    mu_prime = mu + T.tensor(self.noise(),\n",
        "                              dtype=T.float).to(self.actor.device)\n",
        "    self.actor.train()\n",
        "    return mu_prime.cpu().detach().numpy()\n",
        "\n",
        "\n",
        "  def remember(self, state, action, reward, new_state, done):\n",
        "    self.memory.store_transition(state, action, reward, new_state, done)\n",
        "\n",
        "  def learn(self):\n",
        "    if self.memory.mem_cntr < self.batch_size:\n",
        "        return\n",
        "    state, action, reward, new_state, done = \\\n",
        "                                  self.memory.sample_buffer(self.batch_size)\n",
        "\n",
        "    reward = T.tensor(reward, dtype=T.float).to(self.critic.device)\n",
        "    done = T.tensor(done).to(self.critic.device)\n",
        "    new_state = T.tensor(new_state, dtype=T.float).to(self.critic.device)\n",
        "    action = T.tensor(action, dtype=T.float).to(self.critic.device)\n",
        "    state = T.tensor(state, dtype=T.float).to(self.critic.device)\n",
        "\n",
        "    self.target_actor.eval()\n",
        "    self.target_critic.eval()\n",
        "    self.critic.eval()\n",
        "    target_actions = self.target_actor.forward(new_state)\n",
        "    critic_value_ = self.target_critic.forward(new_state, target_actions)\n",
        "    critic_value = self.critic.forward(state, action)\n",
        "\n",
        "    target = []\n",
        "    for j in range(self.batch_size):\n",
        "        target.append(reward[j] + self.gamma*critic_value_[j]*done[j])\n",
        "    target = T.tensor(target).to(self.critic.device)\n",
        "    target = target.view(self.batch_size, 1)\n",
        "\n",
        "    self.critic.train()\n",
        "    self.critic.optimizer.zero_grad()\n",
        "    critic_loss = F.mse_loss(target, critic_value)\n",
        "    critic_loss.backward()\n",
        "    self.critic.optimizer.step()\n",
        "\n",
        "    self.critic.eval()\n",
        "    self.actor.optimizer.zero_grad()\n",
        "    mu = self.actor.forward(state)\n",
        "    self.actor.train()\n",
        "    actor_loss = -self.critic.forward(state, mu)\n",
        "    actor_loss = T.mean(actor_loss)\n",
        "    actor_loss.backward()\n",
        "    self.actor.optimizer.step()\n",
        "\n",
        "    self.update_network_parameters()\n",
        "\n",
        "  def update_network_parameters(self, tau=None):\n",
        "    if tau is None:\n",
        "        tau = self.tau\n",
        "\n",
        "    actor_params = self.actor.named_parameters()\n",
        "    critic_params = self.critic.named_parameters()\n",
        "    target_actor_params = self.target_actor.named_parameters()\n",
        "    target_critic_params = self.target_critic.named_parameters()\n",
        "\n",
        "    critic_state_dict = dict(critic_params)\n",
        "    actor_state_dict = dict(actor_params)\n",
        "    target_critic_dict = dict(target_critic_params)\n",
        "    target_actor_dict = dict(target_actor_params)\n",
        "\n",
        "    for name in critic_state_dict:\n",
        "        critic_state_dict[name] = tau*critic_state_dict[name].clone() + \\\n",
        "                                  (1-tau)*target_critic_dict[name].clone()\n",
        "\n",
        "    self.target_critic.load_state_dict(critic_state_dict)\n",
        "\n",
        "    for name in actor_state_dict:\n",
        "        actor_state_dict[name] = tau*actor_state_dict[name].clone() + \\\n",
        "                                  (1-tau)*target_actor_dict[name].clone()\n",
        "    self.target_actor.load_state_dict(actor_state_dict)\n",
        "\n",
        "    \"\"\"\n",
        "    #Verify that the copy assignment worked correctly\n",
        "    target_actor_params = self.target_actor.named_parameters()\n",
        "    target_critic_params = self.target_critic.named_parameters()\n",
        "    critic_state_dict = dict(target_critic_params)\n",
        "    actor_state_dict = dict(target_actor_params)\n",
        "    print('\\nActor Networks', tau)\n",
        "    for name, param in self.actor.named_parameters():\n",
        "        print(name, T.equal(param, actor_state_dict[name]))\n",
        "    print('\\nCritic Networks', tau)\n",
        "    for name, param in self.critic.named_parameters():\n",
        "        print(name, T.equal(param, critic_state_dict[name]))\n",
        "    input()\n",
        "    \"\"\"\n",
        "\n",
        "  def save_models(self):\n",
        "    self.actor.save_checkpoint()\n",
        "    self.target_actor.save_checkpoint()\n",
        "    self.critic.save_checkpoint()\n",
        "    self.target_critic.save_checkpoint()\n",
        "\n",
        "  def load_models(self):\n",
        "    self.actor.load_checkpoint()\n",
        "    self.target_actor.load_checkpoint()\n",
        "    self.critic.load_checkpoint()\n",
        "    self.target_critic.load_checkpoint()\n",
        "\n",
        "  def check_actor_params(self):\n",
        "    current_actor_params = self.actor.named_parameters()\n",
        "    current_actor_dict = dict(current_actor_params)\n",
        "    original_actor_dict = dict(self.original_actor.named_parameters())\n",
        "    original_critic_dict = dict(self.original_critic.named_parameters())\n",
        "    current_critic_params = self.critic.named_parameters()\n",
        "    current_critic_dict = dict(current_critic_params)\n",
        "    print('Checking Actor parameters')\n",
        "\n",
        "    for param in current_actor_dict:\n",
        "        print(param, T.equal(original_actor_dict[param], current_actor_dict[param]))\n",
        "    print('Checking critic parameters')\n",
        "    for param in current_critic_dict:\n",
        "        print(param, T.equal(original_critic_dict[param], current_critic_dict[param]))\n",
        "    input()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3b9ov4Zffqrt"
      },
      "source": [
        "def plotLearning(scores, filename, x=None, window=5):\n",
        "  N = len(scores)\n",
        "  running_avg = np.empty(N)\n",
        "  for t in range(N):\n",
        "    running_avg[t] = np.mean(scores[max(0, t-window):(t+1)])\n",
        "  if x is None:\n",
        "    x = [i for i in range(N)]\n",
        "  plt.ylabel('Score')\n",
        "  plt.xlabel('Epoch')\n",
        "  plt.plot(x, running_avg)\n",
        "  plt.savefig(filename)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kNzcmIIAna6n"
      },
      "source": [
        "# Grinding Circuit Environment"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0hIvfWSuYNCW"
      },
      "source": [
        "class GrindingCircuit(Env):\n",
        "  def __init__(self):\n",
        "    # Initialize seed\n",
        "    self.seed()\n",
        "\n",
        "    # Set the observation space\n",
        "    self.observation_space = spaces.Box(-np.inf, np.inf, shape=(8,), dtype=np.float32)\n",
        "\n",
        "    # Action is two floats [feedrate, separator speed].\n",
        "    # Feedrate:         -1..+1 actual feedrate / max feedrate\n",
        "    # Separator Speed:  -1..+1 separator speed / max separator speed\n",
        "    self.action_space = spaces.Box(-1, +1, (2,), dtype=np.float32)\n",
        "\n",
        "    # Initialize vars\n",
        "    self.productFineness          = np.inf\n",
        "    self.recirculatingLoad        = np.inf\n",
        "    self.targetproductFineness    = np.inf\n",
        "    self.targetrecirculatingLoad  = np.inf\n",
        "    self.separatorVentSpeed       = np.inf\n",
        "    self.millMainVentSpeed        = np.inf\n",
        "    self.proportionClinker        = np.inf\n",
        "    self.proportionSlag           = np.inf\n",
        "\n",
        "    # Simulation time\n",
        "    self.initialSimulationLength = 1000\n",
        "    self.simulationLength = self.initialSimulationLength\n",
        "\n",
        "    # Trigger reset\n",
        "    self.reset()\n",
        "  \n",
        "  # Make a step in the environment\n",
        "  def step(self, action):\n",
        "    # Update simulation length\n",
        "    self.simulationLength -= 1\n",
        "\n",
        "    # Action values\n",
        "    action = np.clip(action, -1, +1).astype(np.float32)\n",
        "    if action[0] > 0.0:\n",
        "      self.feedRate = (action[0] + 1.0)*50        # 0..100 of max\n",
        "    if action[1] > 0.0:\n",
        "      self.separatorSpeed = (action[1] + 1.0)*50  # 0..100 of max\n",
        "\n",
        "    # TODO - Implement mill simulation script and deliver output states\n",
        "\n",
        "\n",
        "    # Consideres states\n",
        "    # s[0] - Measured product size\n",
        "    # s[1] - Measured recirculating load\n",
        "    # s[2] - Targeted product size\n",
        "    # s[3] - Targeted recirculating load\n",
        "    # s[4] - Setting for the separator speed of the dynamic separator\n",
        "    # s[5] - Setting for the separator speed of the mill main vantilator\n",
        "    # s[6] - Proportion of the fresh feed being clinker (major influence --> hardness)\n",
        "    # s[7] - Proportion of the fresh feed being slag (major influence --> hardness)\n",
        "    state = [\n",
        "      self.productFineness,\n",
        "      self.recirculatingLoad,\n",
        "      self.targetproductFineness,\n",
        "      self.targetrecirculatingLoad,\n",
        "      self.separatorVentSpeed,\n",
        "      self.millMainVentSpeed,\n",
        "      self.proportionClinker,\n",
        "      self.proportionSlag\n",
        "      ]\n",
        "    \n",
        "    # TODO - Implement reward function\n",
        "    reward = 0\n",
        "\n",
        "    # Check if simulation is done\n",
        "    if self.simulationLength <= 0:\n",
        "      done = True\n",
        "    else:\n",
        "      done = False\n",
        "\n",
        "    # Set placeholder for info\n",
        "    info = {}\n",
        "\n",
        "    return np.array(state, dtype=np.float32), reward, done, info\n",
        "\n",
        "  # TODO - Reset the environment\n",
        "  def reset(self):\n",
        "    # Reset simulation time\n",
        "    self.simulationLength = self.initialSimulationLength\n",
        "  \n",
        "    return self.step(np.array([0, 0]))[0]\n",
        "\n",
        "  # Seed\n",
        "  def seed(self, seed=None):\n",
        "    self.np_random, seed = seeding.np_random(seed)\n",
        "    return [seed]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fVZqEl18ykca",
        "outputId": "dc53e0a8-758d-4bf7-f83a-03d9dce19090"
      },
      "source": [
        "# Initialize grinding circuit environment\n",
        "env = GrindingCircuit()\n",
        "# Sample observation space to check if filled\n",
        "env.observation_space.sample()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([ 0.46871677, -1.2871093 , -1.268937  ,  0.24040231, -2.590506  ,\n",
              "       -1.0167024 , -0.06066818,  1.5222704 ], dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vcjVlGyynmvA"
      },
      "source": [
        "# Machine Learning Config"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 449
        },
        "id": "o8uTa1pqEVj9",
        "outputId": "1f1ce9ed-df75-4250-b9c5-a2bf2c14582c"
      },
      "source": [
        "# Initialize the DDPG agent\n",
        "agent = Agent(alpha=0.000025, beta=0.00025, \n",
        "              input_dims=[8], tau=0.001, env=env,\n",
        "              batch_size=40,  layer1_size=1024, layer2_size=1024, n_actions=2)\n",
        "\n",
        "# Option to load models in case of longer learning times\n",
        "#agent.load_models()\n",
        "\n",
        "# Set random seed\n",
        "np.random.seed(0)\n",
        "\n",
        "# Start DDPG deep reinforcement learning\n",
        "# Number of times rl-algorithm is passed\n",
        "epochs = 10\n",
        "score_history = []\n",
        "for i in range(epochs):\n",
        "  obs = env.reset()\n",
        "  done = False\n",
        "  score = 0\n",
        "  while not done:\n",
        "    act = agent.choose_action(obs)\n",
        "    new_state, reward, done, info = env.step(act)\n",
        "    agent.remember(obs, act, reward, new_state, int(done))\n",
        "    agent.learn()\n",
        "    score += reward\n",
        "    obs = new_state\n",
        "    #env.render()\n",
        "  score_history.append(score)\n",
        "\n",
        "  # Option to save model in case of longer learning times\n",
        "  #if i % 25 == 0:\n",
        "  #    agent.save_models()\n",
        "\n",
        "  # Print the results\n",
        "  print('episode ', i, 'score %.2f' % score,\n",
        "        'trailing 100 games avg %.3f' % np.mean(score_history[-100:]))\n",
        "\n",
        "# Plot and save the learning curve\n",
        "filename = 'LunarLander-alpha000025-beta00025-400-300.png'\n",
        "plotLearning(score_history, filename, window=100)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "episode  0 score 0.00 trailing 100 games avg 0.000\n",
            "episode  1 score 0.00 trailing 100 games avg 0.000\n",
            "episode  2 score 0.00 trailing 100 games avg 0.000\n",
            "episode  3 score 0.00 trailing 100 games avg 0.000\n",
            "episode  4 score 0.00 trailing 100 games avg 0.000\n",
            "episode  5 score 0.00 trailing 100 games avg 0.000\n",
            "episode  6 score 0.00 trailing 100 games avg 0.000\n",
            "episode  7 score 0.00 trailing 100 games avg 0.000\n",
            "episode  8 score 0.00 trailing 100 games avg 0.000\n",
            "episode  9 score 0.00 trailing 100 games avg 0.000\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    }
  ]
}